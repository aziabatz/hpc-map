[2024-05-16 14:57:24,251][eval_model][INFO] - ===================
Model Mapping: tensor([3, 2, 3, 3, 1, 1, 2, 3, 0, 0, 0, 0, 1, 1, 2, 2], dtype=torch.int32) Optimal Mapping: tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3], dtype=torch.int32)
Model Reward: -0.4972067177295685 Optimal Reward: -0.10614524781703949
Score (optimal/model ratio): 0.21348313987255096
===================
[2024-05-16 14:57:24,252][eval_model][INFO] - ===================
Model Mapping: tensor([1, 1, 0, 3, 0, 2, 1, 3, 2, 1, 0, 3, 2, 2, 0, 3], dtype=torch.int32) Optimal Mapping: tensor([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3], dtype=torch.int32)
Model Reward: -0.6129032373428345 Optimal Reward: -0.32258063554763794
Score (optimal/model ratio): 0.5263157486915588
===================
[2024-05-16 14:57:24,252][eval_model][INFO] - ===================
Model Mapping: tensor([0, 0, 1, 1, 0, 0, 2, 2, 1, 2, 3, 3, 1, 3, 2, 3], dtype=torch.int32) Optimal Mapping: tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3], dtype=torch.int32)
Model Reward: -0.6653226017951965 Optimal Reward: -0.32258063554763794
Score (optimal/model ratio): 0.4848484694957733
===================
[2024-05-16 14:57:24,253][eval_model][INFO] - ===================
Model Mapping: tensor([1, 0, 1, 1, 0, 1, 0, 2, 0, 2, 2, 2, 3, 3, 3, 3], dtype=torch.int32) Optimal Mapping: tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3], dtype=torch.int32)
Model Reward: -0.5333333611488342 Optimal Reward: -0.20000000298023224
Score (optimal/model ratio): 0.375
===================
[2024-05-16 14:57:24,253][eval_model][INFO] - ===================
Model Mapping: tensor([1, 3, 0, 1, 3, 2, 3, 1, 2, 2, 3, 0, 2, 1, 0, 0], dtype=torch.int32) Optimal Mapping: tensor([1, 1, 2, 2, 1, 1, 2, 3, 0, 0, 3, 3, 0, 0, 3, 2], dtype=torch.int32)
Model Reward: -0.6666666865348816 Optimal Reward: -0.5833333134651184
Score (optimal/model ratio): 0.8749999403953552
===================
[2024-05-16 14:57:24,254][eval_model][INFO] - ===================
Model Mapping: tensor([3, 2, 0, 0, 3, 2, 0, 1, 3, 2, 0, 1, 3, 1, 2, 1], dtype=torch.int32) Optimal Mapping: tensor([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3], dtype=torch.int32)
Model Reward: -0.4236499071121216 Optimal Reward: -0.10614524781703949
Score (optimal/model ratio): 0.25054943561553955
===================
[2024-05-16 14:57:24,254][eval_model][INFO] - ===================
Model Mapping: tensor([0, 0, 1, 1, 1, 0, 0, 1, 2, 2, 2, 2, 3, 3, 3, 3], dtype=torch.int32) Optimal Mapping: tensor([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3], dtype=torch.int32)
Model Reward: -0.800000011920929 Optimal Reward: -0.20000000298023224
Score (optimal/model ratio): 0.25
===================



[2024-05-16 14:57:25,036][rl4co.envs.common.base][INFO] - Loading test dataset from ./data/mpi/16_4.npz
[2024-05-16 14:57:25,144][eval_model][INFO] - ===================
Model Mapping: tensor([0, 1, 3, 1, 3, 3, 3, 2, 0, 1, 0, 0, 2, 2, 2, 1], dtype=torch.int32) Optimal Mapping: tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3], dtype=torch.int32)
Model Reward: -0.5912476778030396 Optimal Reward: -0.10614524781703949
Score (optimal/model ratio): 0.17952755093574524
===================
[2024-05-16 14:57:25,144][eval_model][INFO] - ===================
Model Mapping: tensor([1, 1, 2, 3, 3, 3, 2, 0, 1, 1, 2, 3, 2, 0, 0, 0], dtype=torch.int32) Optimal Mapping: tensor([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3], dtype=torch.int32)
Model Reward: -0.7016128897666931 Optimal Reward: -0.32258063554763794
Score (optimal/model ratio): 0.4597701132297516
===================
[2024-05-16 14:57:25,145][eval_model][INFO] - ===================
Model Mapping: tensor([3, 2, 2, 2, 0, 0, 0, 0, 3, 2, 3, 3, 1, 1, 1, 1], dtype=torch.int32) Optimal Mapping: tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3], dtype=torch.int32)
Model Reward: -0.4838709533214569 Optimal Reward: -0.32258063554763794
Score (optimal/model ratio): 0.6666666865348816
===================
[2024-05-16 14:57:25,145][eval_model][INFO] - ===================
Model Mapping: tensor([0, 1, 1, 2, 0, 2, 0, 2, 0, 2, 3, 3, 1, 3, 1, 3], dtype=torch.int32) Optimal Mapping: tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3], dtype=torch.int32)
Model Reward: -0.6666666865348816 Optimal Reward: -0.20000000298023224
Score (optimal/model ratio): 0.29999998211860657
===================
[2024-05-16 14:57:25,146][eval_model][INFO] - ===================
Model Mapping: tensor([2, 0, 3, 3, 2, 1, 1, 0, 2, 1, 1, 0, 3, 3, 0, 2], dtype=torch.int32) Optimal Mapping: tensor([1, 1, 2, 2, 1, 1, 2, 3, 0, 0, 3, 3, 0, 0, 3, 2], dtype=torch.int32)
Model Reward: -0.7083333134651184 Optimal Reward: -0.5833333134651184
Score (optimal/model ratio): 0.8235294222831726
===================
[2024-05-16 14:57:25,146][eval_model][INFO] - ===================
Model Mapping: tensor([0, 1, 0, 3, 2, 1, 2, 3, 0, 1, 3, 2, 0, 1, 2, 3], dtype=torch.int32) Optimal Mapping: tensor([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3], dtype=torch.int32)
Model Reward: -0.4618249535560608 Optimal Reward: -0.10614524781703949
Score (optimal/model ratio): 0.22983869910240173
===================
[2024-05-16 14:57:25,147][eval_model][INFO] - ===================
Model Mapping: tensor([2, 0, 3, 1, 2, 0, 2, 3, 0, 0, 1, 1, 1, 2, 3, 3], dtype=torch.int32) Optimal Mapping: tensor([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3], dtype=torch.int32)
Model Reward: -0.6666666865348816 Optimal Reward: -0.20000000298023224
Score (optimal/model ratio): 0.29999998211860657
===================