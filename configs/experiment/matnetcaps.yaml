# @package _global_

defaults:
  - override /model: matnet.yaml
  - override /env: mpimap.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml
  - override /logger: wandb.yaml

env:
  num_procs: 12
  num_machines: 3
  max_machine_capacity: 4
  normalize: True

  data_dir: ${paths.root_dir}/data/mpi
  #val_file: all_${env.num_procs}.npz
  test_file: ${env.num_procs}_${env.num_machines}.npz

logger:
  wandb:
    project: "mpi_mapping"
    tags: ["matnet_caps", "${env.name}"]
    group: ${env.name}_procs_${env.num_procs}
    name: matnet_caps-${env.name}-${env.num_procs}_${env.num_machines}_${model.batch_size}_${model.train_data_size}_${trainer.max_epochs}

model:
#  mode: "Random"
  batch_size: 4
  val_batch_size: 4
  test_batch_size: 1
  train_data_size: 1000
  val_data_size: 400
  # test_data_size: 16
  optimizer:
    "Adam"
  optimizer_kwargs:
    lr: 1.25e-4
    weight_decay: 0
  lr_scheduler:
    "MultiStepLR"
    #"ReduceLROnPlateau"
  lr_scheduler_kwargs:
    #milestones: [80, 95]
    milestones: [2, 50, 80, 99]
    # mode: 'min'
    # factor: 0.7
    # patience: 100
    # verbose: True
  feats: "cost_matrix"

trainer:
  max_epochs: 100
  log_every_n_steps: ${model.batch_size}

seed: 1234
